{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183da7bb-bf09-4021-a8ec-6a8446ebfd26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Template de Ingestão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c87de20-f5c0-40b2-ae44-8809635b4bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Pré-requisitos:\n",
    "\n",
    "- Criar cluster com lib tipo PyPI: google-cloud-bigquery.\n",
    "- Executar notebook para inserir \"service account key file\".\n",
    "- Executar notebook de criação de tabelas delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29fe690-c5c3-4726-8e9c-2472b984ec7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, expr, count, row_number, lit, input_file_name\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col as spark_col, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ff631e-1953-43d2-8947-24b3985351d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Instancia logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "#Atribuir variável de embiente\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/tmp/your_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9e9c66-7f8a-48b1-ba8d-75d298514ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Funções Compartilhadas (Utilitárias)\n",
    "\n",
    "#Cria conexao com Storage GCP\n",
    "def conex_gcp():\n",
    "    try:\n",
    "        service_account_key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "        # Configurar as credenciais para acessar o Google Cloud Storage\n",
    "        spark.conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "        spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\", service_account_key_file)\n",
    "\n",
    "        # Exemplo de leitura de dados do GCS\n",
    "        bucket_name = \"lake_data_master\"\n",
    "\n",
    "        return logger.info(\"Connection successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")\n",
    "    \n",
    "\n",
    "#Criar conexão com Big Query\n",
    "def insert_bigquery(dict_metrics):\n",
    "    try:\n",
    "        key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "        \n",
    "        logger.info(\"Opening connection with Big Query\")\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            logger.info(\"Connection successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Connection failed\")\n",
    "\n",
    "        table_id = \"datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\"\n",
    "\n",
    "        logger.info(f\"Inserting data into table Id: {table_id}\")\n",
    "        errors = client.insert_rows_json(table_id, dict_metrics)\n",
    "\n",
    "        if errors == []:\n",
    "            return logger.info(\"Dados inseridos com sucesso.\")\n",
    "        else:\n",
    "            return logger.info(f\"Erros ao inserir dados: {errors}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")    \n",
    "\n",
    "\n",
    "#Realiza o load dos dados no path raiz\n",
    "def load_data_ingestion(path, header, sep):\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\").option(\"header\", header).option(\"sep\", sep).load(path)\n",
    "\n",
    "        if len(df.columns) > 1:\n",
    "\n",
    "            current_date = datetime.now()\n",
    "            dat_carga = current_date.strftime(\"%Y%m%d\")\n",
    "            df_dat = df.withColumn(\"dat_ref_carga\", lit(dat_carga))\n",
    "\n",
    "            logger.info(\"Data loaded successfully\")\n",
    "\n",
    "            return df_dat\n",
    "        \n",
    "        else:\n",
    "            # Lançar uma exceção para indicar que a condição não foi atendida\n",
    "            error_message = \"The DataFrame does not have more than one column. Check the separator used to read the file.\"\n",
    "            df.show(1)\n",
    "            logger.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "    except ValueError as e:\n",
    "        return logger.error(f\"Data load failure: {e}\")\n",
    "    \n",
    "#Verificar dados nulos\n",
    "def check_nulls(df: DataFrame, required_columns: list):\n",
    "    # Verificar se todas as colunas obrigatórias estão presentes no DataFrame\n",
    "    df_columns = set(df.columns)\n",
    "    missing_columns = [col for col in required_columns if col not in df_columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing from the DataFrame: {', '.join(missing_columns)}\")\n",
    "\n",
    "    try:\n",
    "        # Calcular a contagem de valores nulos por coluna\n",
    "        null_counts = df.select([spark_sum(spark_col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "        # Convertendo o resultado para um dicionário\n",
    "        null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "        # Exibindo colunas com valores nulos e suas respectivas contagens\n",
    "        nulls_info = {column: count for column, count in null_counts_dict.items() if count > 0}\n",
    "\n",
    "        # Verificar se há valores nulos nas colunas que não podem conter nulos\n",
    "        invalid_columns = {col: nulls_info[col] for col in required_columns if col in nulls_info and nulls_info[col] > 0}\n",
    "\n",
    "        # Se houver colunas obrigatórias com nulos, lançar exceção\n",
    "        if invalid_columns:\n",
    "            raise ValueError(f\"Error: The following required columns contain null values: {invalid_columns}\")\n",
    "\n",
    "        # Logar outras colunas com nulos, sem lançar exceção\n",
    "        if nulls_info:\n",
    "            logger.info(\"Other columns with null values ​​(not required):\")\n",
    "            for column, count in nulls_info.items():\n",
    "                if column not in required_columns:\n",
    "                    logger.info(f\"Column: {column}, Null count: {count}\")\n",
    "        \n",
    "        logger.info(\"Mandatory columns are valid.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        logger.error(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "#Coletar tempo de execução em segundos\n",
    "def monitor_execution_time(start_time):\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    duration_minutes = duration.total_seconds()\n",
    "\n",
    "    return duration_minutes \n",
    "\n",
    "\n",
    "#Limpar espacos em branco em nome de colunas     \n",
    "def clean_column_names(df):\n",
    "    # Obter os nomes das colunas\n",
    "    column_names = df.columns\n",
    "    \n",
    "    # Criar um dicionário de mapeamento para renomear as colunas\n",
    "    new_column_names = {name: name.strip() for name in column_names}\n",
    "    \n",
    "    # Aplicar as renomeações\n",
    "    for old_name, new_name in new_column_names.items():\n",
    "        if old_name != new_name:  # Verificar se o nome precisa ser alterado\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e77425b-3834-4c2e-b341-4cfe5135e927",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Função template da ingestão\n",
    "def ingestion(db_name, table_name, odate, sep, required_columns):\n",
    "    try:\n",
    "        #Coletar tempo inicial da execução\n",
    "        start_time_total_execution = datetime.now() #métricas\n",
    "        logger.info(f\"Start of execution: {start_time_total_execution}\")\n",
    "\n",
    "        #Gerar conexão com Storage GCP\n",
    "        conex_gcp()\n",
    "\n",
    "        #Realiza load dos dados e inclusão do campo com data de carga\n",
    "        path_load = f\"gs://data-ingestion-bucket-datamaster/table_ingestion_files/{table_name}\"\n",
    "        logger.info(f\"Starting to load data into the path {path_load}/{table_name}_{odate}\")\n",
    "        load_start_time = datetime.now()\n",
    "        df = load_data_ingestion(f\"{path_load}/{table_name}_{odate}.csv\", header=\"true\",sep=sep)\n",
    "        load_total_time = monitor_execution_time(load_start_time )\n",
    "        logger.info(f\"Total time to load data: {load_total_time} seconds\")\n",
    "\n",
    "        # Tamanho dos dados carregados em bytes\n",
    "        data_size_bytes = df.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)\n",
    "        data_size_mb = data_size_bytes / (1024 * 1024)\n",
    "        data_size_mb_formatted = float(f\"{data_size_mb:.2f}\")\n",
    "        logger.info(f\"Size of loaded data: {data_size_mb_formatted} MB\")\n",
    "\n",
    "        #Quantidade de dados carregados\n",
    "        number_lines_loaded = df.count()\n",
    "        logger.info(f\"Number of lines loaded {number_lines_loaded }\")\n",
    "\n",
    "        #Realizando limpeza de espacos em branco no nome das colunas\n",
    "        df_write_clean = clean_column_names(df)\n",
    "\n",
    "        #Realizar validação de campos nulos, obrigatorios e nao obrigatorios\n",
    "        check_nulls(df_write_clean, required_columns)\n",
    "\n",
    "        #Verificar existência da tabela\n",
    "        try:\n",
    "            if spark.catalog.tableExists(f\"{db_name}.{table_name}\"):\n",
    "                logger.info(f\"The table {db_name}.{table_name} exists.\")\n",
    "            else:\n",
    "                logger.info(f\"The table {db_name}.{table_name} does not exist.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while checking the table: {e}\")\n",
    "\n",
    "        #Gravar dados na tabela\n",
    "        write_start_time = datetime.now()\n",
    "        logger.info(f\"Writing data to the table: {table_name}\")\n",
    "        df_write_clean.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .option(\"parquet.file.size\", \"128MB\") \\\n",
    "            .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "            \n",
    "        write_total_time = monitor_execution_time(write_start_time)\n",
    "        logger.info(f\"Data recording execution time: {write_total_time} seconds\")\n",
    "\n",
    "        #Verificar quantidade de dados inseridos\n",
    "        logger.info(f\"Checking amount of data entered\")\n",
    "        current_date = datetime.now()\n",
    "        dat_carga = current_date.strftime(\"%Y%m%d\")\n",
    "        df_verify = spark.read.format(\"delta\").table(f\"{db_name}.{table_name}\").where(col(\"dat_ref_carga\") == dat_carga)\n",
    "        qtd_total_rows_insert = df_verify.count()\n",
    "        num_columns_table = len(df_verify.columns)\n",
    "        logger.info(f\"A total of {qtd_total_rows_insert} rows and a total of {num_columns_table} columns were inserted into the table\")\n",
    "\n",
    "        #Verificar numero de arquivos gerados\n",
    "        logger.info(f\"Checking total generated files\")\n",
    "        df_with_file_name = df.withColumn(\"file_name\", input_file_name())\n",
    "        num_files = df_with_file_name.select(\"file_name\").distinct().count()\n",
    "        logger.info(f\"Total files generated: {num_files}\")\n",
    "\n",
    "        #Coletar tempo final da execução\n",
    "        total_execution = monitor_execution_time(start_time_total_execution )\n",
    "        final_time_total_execution = datetime.now()\n",
    "        logger.info(f\"End of execution: {final_time_total_execution}\")\n",
    "        logger.info(f\"Total execution time: {total_execution}\")\n",
    "\n",
    "        if number_lines_loaded == qtd_total_rows_insert:\n",
    "            alerta = False\n",
    "            logger.info(f\"Table {table_name} ingested successfully\")\n",
    "            logger.info(f\"No alerts regarding validation of entered quantities\")\n",
    "        else:\n",
    "            alerta = True \n",
    "            logger.info(f\"Table {table_name} ingested successfully\")\n",
    "            logger.warning(f\"Check table ingestion, has an ALERT regarding the difference in data found on the load date\")\n",
    "\n",
    "        metricas = [{\n",
    "            \"table_name\": table_name,\n",
    "            \"load_total_time\": load_total_time,\n",
    "            \"number_lines_loaded\": number_lines_loaded,\n",
    "            \"data_size_mb_formatted\": data_size_mb_formatted,\n",
    "            \"write_total_time\": write_total_time,\n",
    "            \"qtd_total_rows_insert\": qtd_total_rows_insert,\n",
    "            \"num_columns_table\": num_columns_table,\n",
    "            \"num_files\": num_files,\n",
    "            \"total_execution\": total_execution,\n",
    "            \"dat_carga\": dat_carga,\n",
    "            \"alerta\": alerta\n",
    "        }]\n",
    "\n",
    "        #Insertir dados na tabela de métricas do Big Query\n",
    "        logger.info(\"Inserting metrics data into Big Query\")\n",
    "        insert_bigquery(metricas )  \n",
    "        \n",
    "        return metricas\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(f\"Error ingesting table {table_name}: {e}\")            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a43a219e-5e5c-4b17-94f3-9fbe05a786a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Execução do Template de Ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458f3df9-9335-4452-921f-9860c9de4bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:31:00,257 - INFO - Start of execution: 2024-09-07 18:31:00.257934\n2024-09-07 18:31:00,260 - INFO - Connection successfully\n2024-09-07 18:31:00,265 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/clientes/clientes_20240909\n2024-09-07 18:31:02,057 - INFO - Data loaded successfully\n2024-09-07 18:31:02,059 - INFO - Total time to load data: 1.79276 seconds\n2024-09-07 18:31:03,223 - INFO - Size of loaded data: 1.08 MB\n2024-09-07 18:31:03,931 - INFO - Number of lines loaded 2000\n2024-09-07 18:31:06,880 - INFO - Mandatory columns are valid.\n2024-09-07 18:31:06,940 - INFO - The table cadastros.clientes exists.\n2024-09-07 18:31:06,941 - INFO - Writing data to the table: clientes\n2024-09-07 18:31:20,289 - INFO - Data recording execution time: 13.347942 seconds\n2024-09-07 18:31:20,292 - INFO - Checking amount of data entered\n2024-09-07 18:31:24,023 - INFO - A total of 2000 rows and a total of 21 columns were inserted into the table\n2024-09-07 18:31:24,025 - INFO - Checking total generated files\n2024-09-07 18:31:25,698 - INFO - Total files generated: 1\n2024-09-07 18:31:25,707 - INFO - End of execution: 2024-09-07 18:31:25.707380\n2024-09-07 18:31:25,708 - INFO - Total execution time: 25.449438\n2024-09-07 18:31:25,710 - INFO - Table clientes ingested successfully\n2024-09-07 18:31:25,712 - INFO - No alerts regarding validation of entered quantities\n2024-09-07 18:31:25,713 - INFO - Inserting metrics data into Big Query\n2024-09-07 18:31:25,719 - INFO - Opening connection with Big Query\n2024-09-07 18:31:25,809 - INFO - Connection successfully\n2024-09-07 18:31:25,811 - INFO - Inserting data into table Id: datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\n2024-09-07 18:31:26,193 - INFO - Dados inseridos com sucesso.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'table_name': 'clientes', 'load_total_time': 1.79276, 'number_lines_loaded': 2000, 'data_size_mb_formatted': 1.08, 'write_total_time': 13.347942, 'qtd_total_rows_insert': 2000, 'num_columns_table': 21, 'num_files': 1, 'total_execution': 25.449438, 'dat_carga': '20240907', 'alerta': False}]\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela clientes\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_clientes = \"clientes\"\n",
    "db_name_clientes = \"cadastros\"\n",
    "odate_clientes = \"20240909\"\n",
    "sep = \";\"\n",
    "required_columns = ['nome', 'cpf'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_clientes = ingestion(db_name_clientes, table_name_clientes, odate_clientes, sep, required_columns)\n",
    "print(metricas_clientes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20e2103-ed66-4a97-bbe4-9491dbbf03ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:31:41,355 - INFO - Start of execution: 2024-09-07 18:31:41.355516\n2024-09-07 18:31:41,360 - INFO - Connection successfully\n2024-09-07 18:31:41,361 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/produtos/produtos_20240909\n2024-09-07 18:31:42,811 - INFO - Data loaded successfully\n2024-09-07 18:31:42,813 - INFO - Total time to load data: 1.449717 seconds\n2024-09-07 18:31:43,405 - INFO - Size of loaded data: 0.02 MB\n2024-09-07 18:31:44,014 - INFO - Number of lines loaded 50\n2024-09-07 18:31:46,383 - INFO - Mandatory columns are valid.\n2024-09-07 18:31:46,495 - INFO - The table cadastros.produtos exists.\n2024-09-07 18:31:46,497 - INFO - Writing data to the table: produtos\n2024-09-07 18:31:54,163 - INFO - Data recording execution time: 7.666322 seconds\n2024-09-07 18:31:54,165 - INFO - Checking amount of data entered\n2024-09-07 18:31:56,872 - INFO - A total of 50 rows and a total of 17 columns were inserted into the table\n2024-09-07 18:31:56,875 - INFO - Checking total generated files\n2024-09-07 18:31:58,116 - INFO - Total files generated: 1\n2024-09-07 18:31:58,118 - INFO - End of execution: 2024-09-07 18:31:58.118041\n2024-09-07 18:31:58,120 - INFO - Total execution time: 16.762517\n2024-09-07 18:31:58,121 - INFO - Table produtos ingested successfully\n2024-09-07 18:31:58,123 - INFO - No alerts regarding validation of entered quantities\n2024-09-07 18:31:58,125 - INFO - Inserting metrics data into Big Query\n2024-09-07 18:31:58,126 - INFO - Opening connection with Big Query\n2024-09-07 18:31:58,183 - INFO - Connection successfully\n2024-09-07 18:31:58,185 - INFO - Inserting data into table Id: datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\n2024-09-07 18:31:58,430 - INFO - Dados inseridos com sucesso.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'table_name': 'produtos', 'load_total_time': 1.449717, 'number_lines_loaded': 50, 'data_size_mb_formatted': 0.02, 'write_total_time': 7.666322, 'qtd_total_rows_insert': 50, 'num_columns_table': 17, 'num_files': 1, 'total_execution': 16.762517, 'dat_carga': '20240907', 'alerta': False}]\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela produtos\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_produtos = \"produtos\"\n",
    "db_name_produtos = \"cadastros\"\n",
    "odate_produtos = \"20240909\"\n",
    "sep = \",\"\n",
    "required_columns = ['id', 'nome', 'descricao', 'categoria'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_produtos = ingestion(db_name_produtos, table_name_produtos, odate_produtos, sep, required_columns)\n",
    "print(metricas_produtos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ca7fb9-88f8-446e-9a9d-7384122576f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 18:32:01,857 - INFO - Start of execution: 2024-09-07 18:32:01.857321\n2024-09-07 18:32:01,861 - INFO - Connection successfully\n2024-09-07 18:32:01,862 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/clientesxprod/clientesxprod_20240909\n2024-09-07 18:32:03,238 - INFO - Data loaded successfully\n2024-09-07 18:32:03,239 - INFO - Total time to load data: 1.375346 seconds\n2024-09-07 18:32:03,884 - INFO - Size of loaded data: 0.58 MB\n2024-09-07 18:32:04,512 - INFO - Number of lines loaded 5000\n2024-09-07 18:32:05,426 - INFO - Mandatory columns are valid.\n2024-09-07 18:32:05,449 - INFO - The table vendas.clientesxprod exists.\n2024-09-07 18:32:05,450 - INFO - Writing data to the table: clientesxprod\n2024-09-07 18:32:12,219 - INFO - Data recording execution time: 6.769233 seconds\n2024-09-07 18:32:12,222 - INFO - Checking amount of data entered\n2024-09-07 18:32:14,876 - INFO - A total of 5000 rows and a total of 5 columns were inserted into the table\n2024-09-07 18:32:14,879 - INFO - Checking total generated files\n2024-09-07 18:32:15,972 - INFO - Total files generated: 1\n2024-09-07 18:32:15,976 - INFO - End of execution: 2024-09-07 18:32:15.976544\n2024-09-07 18:32:15,978 - INFO - Total execution time: 14.119215\n2024-09-07 18:32:15,980 - INFO - Table clientesxprod ingested successfully\n2024-09-07 18:32:15,982 - INFO - No alerts regarding validation of entered quantities\n2024-09-07 18:32:15,984 - INFO - Inserting metrics data into Big Query\n2024-09-07 18:32:15,992 - INFO - Opening connection with Big Query\n2024-09-07 18:32:16,024 - INFO - Connection successfully\n2024-09-07 18:32:16,026 - INFO - Inserting data into table Id: datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\n2024-09-07 18:32:16,289 - INFO - Dados inseridos com sucesso.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'table_name': 'clientesxprod', 'load_total_time': 1.375346, 'number_lines_loaded': 5000, 'data_size_mb_formatted': 0.58, 'write_total_time': 6.769233, 'qtd_total_rows_insert': 5000, 'num_columns_table': 5, 'num_files': 1, 'total_execution': 14.119215, 'dat_carga': '20240907', 'alerta': False}]\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela clientesxprod\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_clientesxprod = \"clientesxprod\"\n",
    "db_name_clientesxprod = \"vendas\"\n",
    "odate_clientesxprod = \"20240909\"\n",
    "sep = \",\"\n",
    "required_columns = ['cliente_id', 'produto_id'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_clientesxprod = ingestion(db_name_clientesxprod, table_name_clientesxprod, odate_clientesxprod, sep, required_columns)\n",
    "print(metricas_clientesxprod)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2226375173427318,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion_bronze_template",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
