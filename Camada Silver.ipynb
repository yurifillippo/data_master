{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ea31a7-bde5-41f1-aa24-ade52ddd9e73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, expr, count, row_number, lit, input_file_name, sha2, current_date, datediff, floor, when, broadcast\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col as spark_col, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType\n",
    "import logging\n",
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23564ab1-c675-46f9-a51c-1dafae0cc74a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Instancia logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "#Atribuir variável de embiente\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/tmp/your_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e999171b-d422-40a7-a567-dbeee86dbdba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Funções Compartilhadas (Utilitárias)\n",
    "\n",
    "#Cria conexao com Storage GCP\n",
    "def conex_gcp():\n",
    "    try:\n",
    "        service_account_key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "        # Configurar as credenciais para acessar o Google Cloud Storage\n",
    "        spark.conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "        spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\", service_account_key_file)\n",
    "\n",
    "        # Exemplo de leitura de dados do GCS\n",
    "        bucket_name = \"lake_data_master\"\n",
    "\n",
    "        return logger.info(\"Connection successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")\n",
    "    \n",
    "\n",
    "#Criar conexão com Big Query\n",
    "def insert_bigquery(dict_metrics):\n",
    "    try:\n",
    "        key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "        \n",
    "        logger.info(\"Opening connection with Big Query\")\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            logger.info(\"Connection successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Connection failed\")\n",
    "\n",
    "        table_id = \"datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\"\n",
    "\n",
    "        logger.info(f\"Inserting data into table Id: {table_id}\")\n",
    "        errors = client.insert_rows_json(table_id, dict_metrics)\n",
    "\n",
    "        if errors == []:\n",
    "            return logger.info(\"Dados inseridos com sucesso.\")\n",
    "        else:\n",
    "            return logger.info(f\"Erros ao inserir dados: {errors}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")    \n",
    "    \n",
    "#Coletar tempo de execução em segundos\n",
    "def monitor_execution_time(start_time):\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    duration_minutes = duration.total_seconds()\n",
    "\n",
    "    return duration_minutes \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fd302a7-d179-4724-972a-48fd27b89306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 22:07:02,791 - INFO - Connection successfully\n"
     ]
    }
   ],
   "source": [
    "conex_gcp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606cdd29-52b6-4d93-8f59-0c3f2b03fd02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tabela Silver de Limite de Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40bfb38d-47a8-4006-955f-aa6aa7536bf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dat_carga = \"20240922\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8668e964-539c-4aec-aab7-b71d4d057f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 22:07:06,516 - INFO - Start of execution: 2024-09-22 22:07:06.516033\n2024-09-22 22:07:06,517 - INFO - Carregando tabela b_cad.clientes\n2024-09-22 22:07:06,833 - INFO - Tabela cadastros.clientes carregada com sucesso\n2024-09-22 22:07:06,834 - INFO - Carregando tabela b_cad.produtos\n2024-09-22 22:07:07,038 - INFO - Tabela cadastros.produtos carregada com sucesso\n2024-09-22 22:07:07,040 - INFO - Carregando tabela b_vend.clientesxprod\n2024-09-22 22:07:07,185 - INFO - Tabela vendas.clientesxprod carregada com sucesso\n2024-09-22 22:07:07,186 - INFO - [* Metrics *] - Total time to load data: 0.667971 seconds\n2024-09-22 22:07:10,206 - INFO - Size of loaded data: 1.85 MB\n2024-09-22 22:07:12,209 - INFO - [* Metrics *] -  Numero total de linhas: 27976\n2024-09-22 22:07:12,211 - INFO - Writing data to the table: clie_limit\n2024-09-22 22:07:20,420 - INFO - [* Metrics *] -  Data recording execution time: 8.209084 seconds\n2024-09-22 22:07:20,422 - INFO - Checking amount of data entered\n2024-09-22 22:07:23,032 - INFO - [* Metrics *] -  A total of 27976 rows and a total of 13 columns were inserted into the table\n2024-09-22 22:07:23,033 - INFO - Checking total generated files\n2024-09-22 22:07:24,060 - INFO - [* Metrics *] -  Total files generated: 2\n2024-09-22 22:07:24,062 - INFO - End of execution: 2024-09-22 22:07:24.062519\n2024-09-22 22:07:24,064 - INFO - [* Metrics *] -  Total execution time: 17.546423\n2024-09-22 22:07:24,065 - INFO - Table clie_limit ingested successfully\n2024-09-22 22:07:24,066 - INFO - No alerts regarding validation of entered quantities\n2024-09-22 22:07:24,068 - INFO - Inserting metrics data into Big Query\n2024-09-22 22:07:24,069 - INFO - Deletando arquivos do path de origem\n2024-09-22 22:07:24,070 - INFO - Successfully Completed\n2024-09-22 22:07:24,071 - INFO - Total time to execution: 17.555806 seconds\n"
     ]
    }
   ],
   "source": [
    "# Coletar tempo inicial da execução\n",
    "start_time_total_execution = datetime.now()\n",
    "logger.info(f\"Start of execution: {start_time_total_execution}\")\n",
    "\n",
    "# Carregar tabela de clientes\n",
    "logger.info(\"Carregando tabela b_cad.clientes\")\n",
    "load_start_time = datetime.now()\n",
    "df_clientes = spark.read.table(\"b_cad.clientes\").select('client_id', 'cpf', 'rg', 'data_nascimento', 'est_civil', 'genero', 'estado', 'renda', 'tp_cliente').where(col(\"dat_ref_carga\") == dat_carga)\n",
    "logger.info(f\"Tabela cadastros.clientes carregada com sucesso\")\n",
    "\n",
    "# Carregar tabela de produtos\n",
    "logger.info(\"Carregando tabela b_cad.produtos\")\n",
    "df_produtos = spark.read.table(\"b_cad.produtos\").select('produto_id', 'nome', 'categoria', 'limite_credito').where(col(\"dat_ref_carga\") == dat_carga)\n",
    "logger.info(f\"Tabela cadastros.produtos carregada com sucesso\")\n",
    "\n",
    "# Carregar tabela de clientesxprod\n",
    "logger.info(\"Carregando tabela b_vend.clientesxprod\")\n",
    "df_clientesxprod = spark.read.table(\"b_vend.clientesxprod\").where(col(\"dat_ref_carga\") == dat_carga)\n",
    "logger.info(f\"Tabela vendas.clientesxprod carregada com sucesso\")\n",
    "\n",
    "# Tempo total de load das tabelas\n",
    "load_total_time = monitor_execution_time(load_start_time)\n",
    "logger.info(f\"[* Metrics *] - Total time to load data: {load_total_time} seconds\")\n",
    "\n",
    "# Tamanho dos dados carregados em bytes\n",
    "df_produtos_size_bytes = df_produtos.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)\n",
    "df_clientes_size_bytes = df_clientes.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)\n",
    "df_clientesxprod_size_bytes = df_clientesxprod.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)\n",
    "\n",
    "data_size_bytes = (df_produtos_size_bytes + df_clientes_size_bytes + df_clientesxprod_size_bytes)\n",
    "\n",
    "data_size_mb = data_size_bytes / (1024 * 1024)\n",
    "data_size_mb_formatted = float(f\"{data_size_mb:.2f}\")\n",
    "logger.info(f\"[* Metrics *] - Size of loaded data: {data_size_mb_formatted} MB\")\n",
    "\n",
    "# Join entre clientesxprod e clientes\n",
    "df_clientesxprod = df_clientesxprod.withColumnRenamed('cliente_id', 'client_id')\n",
    "df_join_clientesxprod_cli = df_clientesxprod.join(df_clientes, on='client_id', how='left')\n",
    "\n",
    "# Join entre df_join_clientesxprod_cli e tabela de produtos\n",
    "df_final_join = df_join_clientesxprod_cli.join(broadcast(df_produtos), on='produto_id', how='left')\n",
    "\n",
    "#Criar novas colunas\n",
    "df_final = df_final_join.withColumn(\"num_doc\", sha2(col(\"cpf\"), 256)) \\\n",
    "                        .withColumn(\"data_nascimento\", col(\"data_nascimento\").cast(DateType())) \\\n",
    "                        .withColumn(\"idade\", floor(datediff(current_date(), col(\"data_nascimento\")) / 365.25).cast(\"string\")) \\\n",
    "                        .withColumn(\n",
    "                        \"classe_renda\", when(col(\"renda\") <= 521, \"Classe E\")\n",
    "                        .when((col(\"renda\") > 521) & (col(\"renda\") <= 1042), \"Classe D\")\n",
    "                        .when((col(\"renda\") > 1042) & (col(\"renda\") <= 4427), \"Classe C\")\n",
    "                        .when((col(\"renda\") > 4427) & (col(\"renda\") <= 8856), \"Classe B\")\n",
    "                        .otherwise(\"Classe A\")) \\\n",
    "                        .withColumn(\"prod_contratado\", col(\"nome\")) \\\n",
    "                        .withColumn(\"valor_prod_contratado\", col(\"valor_aquisicao\")) \\\n",
    "                        .withColumn(\"dat_ref_carga\", lit(dat_carga)) \n",
    "\n",
    "\n",
    "cols_df = ['num_doc','est_civil','idade','tp_cliente','classe_renda','genero','estado','renda','prod_contratado','valor_prod_contratado','categoria','limite_credito','dat_ref_carga']\n",
    "\n",
    "#Definir apenas registros com limite de crédito\n",
    "df_insert = df_final.select(cols_df).where(col(\"limite_credito\").isNotNull())\n",
    "\n",
    "#Verificando quantidade de registros após o filtro de limite de crédito\n",
    "qtd_registros = df_insert.count()\n",
    "logger.info(f\"[* Metrics *] -  Numero total de linhas: {qtd_registros}\")\n",
    "\n",
    "#Inserir dados na tabela clie_limit\n",
    "write_start_time = datetime.now()\n",
    "db_name = \"s_vend\"\n",
    "table_name = \"clie_limit\"\n",
    "logger.info(f\"Writing data to the table: {table_name}\")\n",
    "df_insert.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"parquet.file.size\", \"128MB\") \\\n",
    "    .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "    \n",
    "write_total_time = monitor_execution_time(write_start_time)\n",
    "logger.info(f\"[* Metrics *] - Data recording execution time: {write_total_time} seconds\")\n",
    "\n",
    "#Verificar quantidade de dados inseridos\n",
    "logger.info(f\"Checking amount of data entered\")\n",
    "current_date = datetime.now()\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").table(f\"{db_name}.{table_name}\").where(col(\"dat_ref_carga\") == dat_carga)\n",
    "qtd_total_rows_insert = df_verify.count()\n",
    "num_columns_table = len(df_verify.columns)\n",
    "logger.info(f\"[* Metrics *] - A total of {qtd_total_rows_insert} rows and a total of {num_columns_table} columns were inserted into the table\")\n",
    "\n",
    "#Verificar numero de arquivos gerados\n",
    "logger.info(f\"Checking total generated files\")\n",
    "df_with_file_name = df_verify.withColumn(\"file_name\", input_file_name())\n",
    "num_files = df_with_file_name.select(\"file_name\").distinct().count()\n",
    "logger.info(f\"[* Metrics *] - Total files generated: {num_files}\")\n",
    "\n",
    "#Coletar tempo final da execução\n",
    "total_execution = monitor_execution_time(start_time_total_execution)\n",
    "final_time_total_execution = datetime.now()\n",
    "logger.info(f\"End of execution: {final_time_total_execution}\")\n",
    "logger.info(f\"[* Metrics *] - Total execution time: {total_execution}\")\n",
    "\n",
    "if qtd_registros == qtd_total_rows_insert:\n",
    "    alerta = False\n",
    "    logger.info(f\"Table {table_name} ingested successfully\")\n",
    "    logger.info(f\"No alerts regarding validation of entered quantities\")\n",
    "else:\n",
    "    alerta = True \n",
    "    logger.info(f\"Table {table_name} ingested successfully\")\n",
    "    logger.warning(f\"Check table ingestion, has an ALERT regarding the difference in data found on the load date\")\n",
    "\n",
    "\n",
    "metricas = [{\n",
    "    \"table_name\": table_name, #Nome da tabela ok\n",
    "    \"load_total_time\": load_total_time, #Tempo total de load dos dados brutos ok\n",
    "    \"qtd_total_rows_insert\": qtd_registros, #Quantidade total de linhas inseridas ok\n",
    "    \"write_total_time\": write_total_time, #Tempo total de escrita na tabela delta ok\n",
    "    \"data_size_mb_formatted\": data_size_mb_formatted, #Tamanho em MB dos dados brutos carregados\n",
    "    \"qts_total_rows_insert_verify\": qtd_total_rows_insert, #Quantidade de linhas ao ler a tabela com o odate ok\n",
    "    \"num_columns_table\": num_columns_table, #Numero de colunas da tabelaok\n",
    "    \"num_files\": num_files, #Número de arquivos parquet gerados ok\n",
    "    \"total_execution\": total_execution, #Tempo total de execução do template de ingestão\n",
    "    \"dat_carga\": dat_carga, #Data de execução\n",
    "    \"alerta\": alerta #Alerta em divergência de quantidade de dados inseridos no o mesmo odate\n",
    "}]\n",
    "\n",
    "#Insertir dados na tabela de métricas do Big Query\n",
    "logger.info(\"Inserting metrics data into Big Query\")\n",
    "#insert_bigquery(metricas)  \n",
    "\n",
    "logger.info(\"Deletando arquivos do path de origem\")\n",
    "#delete_files(bucket_name, blob_name)\n",
    "\n",
    "logger.info(\"Successfully Completed\")\n",
    "\n",
    "#####################\n",
    "\n",
    "# Tempo total de execução\n",
    "execution_total_time = monitor_execution_time(start_time_total_execution)\n",
    "logger.info(f\"Total time to execution: {execution_total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2894b10e-6e5c-4180-94de-fa7992d3a336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[92]: [{'table_name': 'clie_limit',\n  'load_total_time': 0.667971,\n  'qtd_total_rows_insert': 27976,\n  'write_total_time': 8.209084,\n  'data_size_mb_formatted': 1.85,\n  'qts_total_rows_insert_verify': 27976,\n  'num_columns_table': 13,\n  'num_files': 2,\n  'total_execution': 17.546423,\n  'dat_carga': '20240922',\n  'alerta': False}]"
     ]
    }
   ],
   "source": [
    "metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70c4900-8b21-42f1-ac04-c7781406ae85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "Validar métricas\n",
    "Validar tempos e o que é necessário medir\n",
    "Criar tabela de métricas da silver\n",
    "Testar insert dos dados no big query\n",
    "Verificar coleta de métricas do spark no ingestion e na silver"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada Silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
