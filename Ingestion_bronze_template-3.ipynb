{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183da7bb-bf09-4021-a8ec-6a8446ebfd26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Template de Ingestão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c87de20-f5c0-40b2-ae44-8809635b4bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Pré-requisitos:\n",
    "\n",
    "- Criar cluster com lib tipo PyPI: google-cloud-bigquery.\n",
    "- Executar notebook para inserir \"service account key file\".\n",
    "- Executar notebook de criação de tabelas delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c097c4e-b11b-469f-a103-655429f7ad58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: sparkmeasure in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c07b83d8-f685-4384-a8d5-4ccc889ed8a2/lib/python3.9/site-packages (0.24.0)\nCollecting latest\n  Downloading latest-0.6.0.tar.gz (12 kB)\nRequirement already satisfied: pyparsing>=2.2.0 in /databricks/python3/lib/python3.9/site-packages (from latest) (3.0.4)\nCollecting pyyaml>=5.0.0\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\nBuilding wheels for collected packages: latest\n  Building wheel for latest (setup.py): started\n  Building wheel for latest (setup.py): finished with status 'done'\n  Created wheel for latest: filename=latest-0.6.0-py3-none-any.whl size=7816 sha256=3e665a94051ab5b5844d71cccaaa7d430a1434f061fbf42f940b930212a11f4f\n  Stored in directory: /root/.cache/pip/wheels/2e/8f/d4/99ce627fbf3cd8fd2de96301507031233cc3ca95778759517b\nSuccessfully built latest\nInstalling collected packages: pyyaml, latest\nSuccessfully installed latest-0.6.0 pyyaml-6.0.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install sparkmeasure latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29fe690-c5c3-4726-8e9c-2472b984ec7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, expr, count, row_number, lit, input_file_name, when, trim\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col as spark_col, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from sparkmeasure import StageMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ff631e-1953-43d2-8947-24b3985351d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Instancia logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "#Atribuir variável de embiente\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/tmp/your_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9e9c66-7f8a-48b1-ba8d-75d298514ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Funções Compartilhadas (Utilitárias)\n",
    "\n",
    "#Cria conexao com Storage GCP\n",
    "def conex_gcp():\n",
    "    \"\"\"\n",
    "    Creates connection to Google Cloud by reading service accounts key file.\n",
    "\n",
    "    Returns:\n",
    "        string: Connection status log\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        service_account_key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "        # Configurar as credenciais para acessar o Google Cloud Storage\n",
    "        spark.conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "        spark.conf.set(\"google.cloud.auth.service.account.json.keyfile\", service_account_key_file)\n",
    "\n",
    "        # Exemplo de leitura de dados do GCS\n",
    "        bucket_name = \"lake_data_master\"\n",
    "\n",
    "        return logger.info(\"Connection successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")\n",
    "    \n",
    "\n",
    "#Criar conexão com Big Query\n",
    "def insert_bigquery(dict_metrics):\n",
    "    \"\"\"\n",
    "    Inserts metrics data into Big Query.\n",
    "\n",
    "    Args:\n",
    "        dict_metrics (dict): Dictionary with metrics extracted from engine execution.\n",
    "\n",
    "    Returns:\n",
    "        string: Insert status log\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "        \n",
    "        logger.info(\"Opening connection with Big Query\")\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            logger.info(\"Connection successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Connection failed\")\n",
    "\n",
    "        table_id = \"datamaster01.ingestion_metrics_data_master.ingestion_metrics_data_lake\"\n",
    "\n",
    "        logger.info(f\"Inserting data into table Id: {table_id}\")\n",
    "        errors = client.insert_rows_json(table_id, dict_metrics)\n",
    "\n",
    "        if errors == []:\n",
    "            return logger.info(\"Data entered successfully.\")\n",
    "        else:\n",
    "            return logger.info(f\"Errors when entering data: {errors}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return logger.error(\"Connection failed\")    \n",
    "\n",
    "\n",
    "#Realiza o load dos dados no path raiz\n",
    "def load_data_ingestion(path, header, sep):\n",
    "    \"\"\"\n",
    "    Loads the raw data into the source.\n",
    "\n",
    "    Args:\n",
    "        path (string): stage path where the raw data is located..\n",
    "        header (string/boolean): true or false\n",
    "        sep (string): Data separator within the file.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: O resultado da multiplicação.\n",
    "\n",
    "    Example:\n",
    "        +-----+-----+---------------+\n",
    "        |nome |idade|cidade         |\n",
    "        +-----+-----+---------------+\n",
    "        |Alice|30   |São Paulo      |\n",
    "        |Bob  |25   |Rio de Janeiro |\n",
    "        |Carol|28   |Belo Horizonte |\n",
    "        +-----+-----+---------------+\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        df = spark.read.format(\"csv\").option(\"header\", header).option(\"sep\", sep).load(path)\n",
    "\n",
    "        if len(df.columns) > 1:\n",
    "\n",
    "            current_date = datetime.now()\n",
    "            dat_carga = current_date.strftime(\"%Y%m%d\")\n",
    "            df_dat = df.withColumn(\"dat_ref_carga\", lit(dat_carga))\n",
    "\n",
    "            logger.info(\"Data loaded successfully\")\n",
    "\n",
    "            return df_dat\n",
    "        \n",
    "        else:\n",
    "            # Lançar uma exceção para indicar que a condição não foi atendida\n",
    "            error_message = \"The DataFrame does not have more than one column. Check the separator used to read the file.\"\n",
    "            df.show(1)\n",
    "            logger.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "    except ValueError as e:\n",
    "        return logger.error(f\"Data load failure: {e}\")\n",
    "    \n",
    "#Verificar dados nulos\n",
    "def check_nulls(df: DataFrame, required_columns: list):\n",
    "    \"\"\"\n",
    "    Checks for null values ​​in columns that should not have null values.\n",
    "\n",
    "    Args:\n",
    "        df (Dataframe): Dataframe loaded with raw data.\n",
    "        required_columns (list): List with the name of columns that must not have null values.\n",
    "\n",
    "    Returns:\n",
    "        string: Status log\n",
    "    \"\"\"\n",
    "    # Verificar se todas as colunas obrigatórias estão presentes no DataFrame\n",
    "    df_columns = set(df.columns)\n",
    "    missing_columns = [col for col in required_columns if col not in df_columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing from the DataFrame: {', '.join(missing_columns)}\")\n",
    "\n",
    "    try:\n",
    "        # Calcular a contagem de valores nulos por coluna\n",
    "        null_counts = df.select([spark_sum(spark_col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "        # Convertendo o resultado para um dicionário\n",
    "        null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "        # Exibindo colunas com valores nulos e suas respectivas contagens\n",
    "        nulls_info = {column: count for column, count in null_counts_dict.items() if count > 0}\n",
    "\n",
    "        # Verificar se há valores nulos nas colunas que não podem conter nulos\n",
    "        invalid_columns = {col: nulls_info[col] for col in required_columns if col in nulls_info and nulls_info[col] > 0}\n",
    "\n",
    "        # Se houver colunas obrigatórias com nulos, lançar exceção\n",
    "        if invalid_columns:\n",
    "            raise ValueError(f\"Error: The following required columns contain null values: {invalid_columns}\")\n",
    "\n",
    "        # Logar outras colunas com nulos, sem lançar exceção\n",
    "        if nulls_info:\n",
    "            logger.info(\"Other columns with null values ​​(not required):\")\n",
    "            for column, count in nulls_info.items():\n",
    "                if column not in required_columns:\n",
    "                    logger.info(f\"Column: {column}, Null count: {count}\")\n",
    "        \n",
    "        logger.info(\"Mandatory columns are valid.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        logger.error(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "#Coletar tempo de execução em segundos\n",
    "def monitor_execution_time(start_time):\n",
    "    \"\"\"\n",
    "    Receives the initial time of the operation to be monitored and informs the total execution time in seconds.\n",
    "\n",
    "    Args:\n",
    "        start_time (datetime): datetime.datetime(2024, 9, 12, 1, 20, 22, 940241)\n",
    "    \n",
    "    Returns:\n",
    "        float: 0.000142.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    duration_minutes = duration.total_seconds()\n",
    "\n",
    "    return duration_minutes \n",
    "\n",
    "\n",
    "#Limpar espacos em branco em nome de colunas     \n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Clear blank spaces that may appear in the column field\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Dataframe loaded with raw data.\n",
    "\n",
    "    Returns:\n",
    "        df (DataFrame): Dataframe with column names without blanks.\n",
    "\n",
    "    \"\"\"\n",
    "    # Obter os nomes das colunas\n",
    "    column_names = df.columns\n",
    "    \n",
    "    # Criar um dicionário de mapeamento para renomear as colunas\n",
    "    new_column_names = {name: name.strip() for name in column_names}\n",
    "    \n",
    "    # Aplicar as renomeações\n",
    "    for old_name, new_name in new_column_names.items():\n",
    "        if old_name != new_name:  # Verificar se o nome precisa ser alterado\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Deletar arquivos da ingestão após inclusão dos dados na tabela. \n",
    "def delete_files(bucket_name, blob_name):\n",
    "    \"\"\"\n",
    "    Delete files from ingestion after adding data to the table.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (string): Bucket name.\n",
    "        blob_name (string): Name of the path within the bucket.\n",
    "\n",
    "    Returns:\n",
    "        string: Status log\n",
    "    \"\"\"\n",
    "\n",
    "    service_account_key_file = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "    # Criar cliente de storage\n",
    "    storage_client = storage.Client.from_service_account_json(service_account_key_file)\n",
    "    \n",
    "    # Obter referência ao bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Listar todos os arquivos (blobs) com o prefixo especificado\n",
    "    blobs = bucket.list_blobs(prefix=blob_name)\n",
    "        \n",
    "    # Deletar cada arquivo encontrado\n",
    "    for blob in blobs:\n",
    "        # Verifica se o blob é um arquivo e não uma pasta\n",
    "        if not blob.name.endswith('/'):\n",
    "            blob.delete()\n",
    "            print(f\"File {blob.name} successfully deleted.\")\n",
    "    \n",
    "    print(\"All files were successfully deleted.\")\n",
    "    \n",
    "    logger.info(f\"File {blob_name} successfully deleted from bucket {bucket_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e77425b-3834-4c2e-b341-4cfe5135e927",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Função template da ingestão\n",
    "def ingestion(db_name, table_name, sep, required_columns):\n",
    "    \"\"\"\n",
    "    Template de ingestão para arquivos csv.\n",
    "\n",
    "    Args:\n",
    "        db_name (string): Name of the table database/catalog.\n",
    "        table_name (string): Table name.\n",
    "        sep (string): Data separator within the file.\n",
    "        required_columns (list):  List with the name of columns that must not have null values.\n",
    "\n",
    "    Returns:\n",
    "        string: Status log.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializa o StageMetrics\n",
    "    stagemetrics = StageMetrics(spark)\n",
    "\n",
    "    # Começa a coleta de métricas\n",
    "    stagemetrics.begin()\n",
    "\n",
    "    try:\n",
    "        #Coletar tempo inicial da execução\n",
    "        start_time_total_execution = datetime.now()\n",
    "        logger.info(f\"Start of execution: {start_time_total_execution}\")\n",
    "\n",
    "        #Gerar conexão com Storage GCP\n",
    "        conex_gcp()\n",
    "        \n",
    "        #Definição de variáveis\n",
    "        bucket_name = \"data-ingestion-bucket-datamaster\"\n",
    "        blob_name = f\"table_ingestion_files/{table_name}/\"\n",
    "\n",
    "        #Realiza load dos dados e inclusão do campo com data de carga\n",
    "        path_load = f\"gs://{bucket_name}/{blob_name }\"\n",
    "        logger.info(f\"Starting to load data into the path {path_load}/\")\n",
    "        load_start_time = datetime.now()\n",
    "        df = load_data_ingestion(f\"{path_load}\", header=\"true\",sep=sep)\n",
    "        load_total_time = monitor_execution_time(load_start_time)\n",
    "        logger.info(f\"Total time to load data: {load_total_time} seconds\")\n",
    "\n",
    "        # Tamanho dos dados carregados em bytes\n",
    "        data_size_bytes = df.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)\n",
    "        data_size_mb = data_size_bytes / (1024 * 1024)\n",
    "        data_size_mb_formatted = float(f\"{data_size_mb:.2f}\")\n",
    "        logger.info(f\"Size of loaded data: {data_size_mb_formatted} MB\")\n",
    "\n",
    "        #Quantidade de dados carregados\n",
    "        number_lines_loaded = df.count()\n",
    "        logger.info(f\"Number of lines loaded {number_lines_loaded }\")\n",
    "\n",
    "        #Transformando todos os campos com a string Null ou null\n",
    "        logger.info(f\"Verificando colunas com string Null ou null e substituindo por None\") \n",
    "        df_transf = df\n",
    "\n",
    "        # Substitui 'NULL' e 'null' por None em todas as colunas, removendo também espaços extras\n",
    "        for column in df.columns:\n",
    "            df_transf = df_transf.withColumn(\n",
    "                column, \n",
    "                when(trim(col(column)).isin(\"NULL\", \"null\"), None).otherwise(col(column)))\n",
    "\n",
    "        #Realizando limpeza de espacos em branco no nome das colunas\n",
    "        df_write_clean = clean_column_names(df_transf)\n",
    "\n",
    "        #Realizar validação de campos nulos, obrigatorios e nao obrigatorios\n",
    "        check_nulls(df_write_clean, required_columns)\n",
    "\n",
    "        #Verificar existência da tabela\n",
    "        try:\n",
    "            if spark.catalog.tableExists(f\"{db_name}.{table_name}\"):\n",
    "                logger.info(f\"The table {db_name}.{table_name} exists.\")\n",
    "            else:\n",
    "                logger.info(f\"The table {db_name}.{table_name} does not exist.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while checking the table: {e}\")\n",
    "\n",
    "        #Gravar dados na tabela\n",
    "        write_start_time = datetime.now()\n",
    "        logger.info(f\"Writing data to the table: {table_name}\")\n",
    "        df_write_clean.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .option(\"parquet.file.size\", \"128MB\") \\\n",
    "            .saveAsTable(f\"{db_name}.{table_name}\")\n",
    "            \n",
    "        write_total_time = monitor_execution_time(write_start_time)\n",
    "        logger.info(f\"Data recording execution time: {write_total_time} seconds\")\n",
    "\n",
    "        #Verificar quantidade de dados inseridos\n",
    "        logger.info(f\"Checking amount of data entered\")\n",
    "        current_date = datetime.now()\n",
    "        dat_carga = current_date.strftime(\"%Y%m%d\")\n",
    "        df_verify = spark.read.format(\"delta\").table(f\"{db_name}.{table_name}\").where(col(\"dat_ref_carga\") == dat_carga)\n",
    "        qtd_total_rows_insert = df_verify.count()\n",
    "        num_columns_table = len(df_verify.columns)\n",
    "        logger.info(f\"A total of {qtd_total_rows_insert} rows and a total of {num_columns_table} columns were inserted into the table\")\n",
    "\n",
    "        #Verificar numero de arquivos gerados\n",
    "        logger.info(f\"Checking total generated files\")\n",
    "        df_with_file_name = df.withColumn(\"file_name\", input_file_name())\n",
    "        num_files = df_with_file_name.select(\"file_name\").distinct().count()\n",
    "        logger.info(f\"Total files generated: {num_files}\")\n",
    "\n",
    "        #Coletar tempo final da execução\n",
    "        total_execution = monitor_execution_time(start_time_total_execution )\n",
    "        final_time_total_execution = datetime.now()\n",
    "        logger.info(f\"End of execution: {final_time_total_execution}\")\n",
    "        logger.info(f\"Total execution time: {total_execution}\")\n",
    "\n",
    "        if number_lines_loaded == qtd_total_rows_insert:\n",
    "            alerta = False\n",
    "            logger.info(f\"Table {table_name} ingested successfully\")\n",
    "            logger.info(f\"No alerts regarding validation of entered quantities\")\n",
    "        else:\n",
    "            alerta = True \n",
    "            logger.info(f\"Table {table_name} ingested successfully\")\n",
    "            logger.warning(f\"Check table ingestion, has an ALERT regarding the difference in data found on the load date\")\n",
    "\n",
    "        # Finaliza a coleta de métricas\n",
    "        stagemetrics.end()\n",
    "\n",
    "        # Cria um DataFrame com as métricas coletadas\n",
    "        stagemetrics_df = stagemetrics.create_stagemetrics_DF()\n",
    "\n",
    "        # Realiza agregações para todas as métricas, consolidando tudo\n",
    "        consolidated_metrics = stagemetrics_df.agg(\n",
    "            {\n",
    "                'stageDuration': \"sum\",\n",
    "                'executorRunTime': \"sum\",\n",
    "                'executorCpuTime': \"sum\",\n",
    "                'jvmGCTime': \"sum\",\n",
    "                'numTasks': \"sum\",\n",
    "                'recordsRead': \"sum\",\n",
    "                'bytesRead': \"sum\",\n",
    "                'recordsWritten': \"sum\",\n",
    "                'bytesWritten': \"sum\",\n",
    "                'diskBytesSpilled': \"sum\",\n",
    "                'memoryBytesSpilled': \"sum\",\n",
    "                'shuffleFetchWaitTime': \"sum\",\n",
    "                'peakExecutionMemory': \"sum\"}\n",
    "        )\n",
    "\n",
    "        # Converte o DataFrame em uma lista de dicionários\n",
    "        metricas_list = consolidated_metrics.collect()  # Coleta os dados do DataFrame\n",
    "        metricas_list = [row.asDict() for row in metricas_list]  # Converte cada linha em um dicionário\n",
    "\n",
    "        metricas = [{\n",
    "            \"table_name\": table_name, #Nome da tabela.\n",
    "            \"load_total_time\": load_total_time, #Tempo total de load dos dados brutos.\n",
    "            \"number_lines_loaded\": number_lines_loaded, #Número de linhas na tabela com o date em execução.\n",
    "            \"data_size_mb_formatted\": data_size_mb_formatted, #Tamanho em MB dos dados brutos carregados.\n",
    "            \"write_total_time\": write_total_time, #Tempo total de escrita na tabela delta.\n",
    "            \"qtd_total_rows_insert\": qtd_total_rows_insert, #Quantidade total de linhas inseridas.\n",
    "            \"num_columns_table\": num_columns_table, #Numero de colunas da tabela.\n",
    "            \"num_files\": num_files, #Número de arquivos parquet gerados.\n",
    "            \"total_execution\": total_execution, #Tempo total de execução do template de ingestão.\n",
    "            \"dat_carga\": dat_carga, #Data de execução.\n",
    "            \"alerta\": alerta, #Alerta em divergência de quantidade de dados inseridos no o mesmo odate.\n",
    "\n",
    "            #spark measure metrics\n",
    "            'sparkM_stageDuration': sparkmeasure_metrics[0]['sum(stageDuration)'], #Indica o tempo total que o estágio levou para ser executado.\n",
    "            'sparkM_executorRunTime': sparkmeasure_metrics[0]['sum(executorRunTime)'], #Mostra quanto tempo o executor passou realmente executando a tarefa.\n",
    "            'sparkM_executorCpuTime': sparkmeasure_metrics[0]['sum(executorCpuTime)'], #Refere-se ao tempo que a CPU efetivamente gastou processando a tarefa.\n",
    "            'sparkM_jvmGCTime': sparkmeasure_metrics[0]['sum(jvmGCTime)'], #Tempo gasto em coleta de lixo.\n",
    "            'sparkM_numTasks': sparkmeasure_metrics[0]['sum(numTasks)'], #Indica quantas tarefas foram executadas em paralelo.\n",
    "            'sparkM_recordsRead': sparkmeasure_metrics[0]['sum(recordsRead)'], #Informam sobre a quantidade de dados lidos.\n",
    "            'sparkM_bytesRead': sparkmeasure_metrics[0]['sum(bytesRead)'], #Informam sobre a quantidade de dados lidos.\n",
    "            'sparkM_recordsWritten': sparkmeasure_metrics[0]['sum(recordsWritten)'], #Analisam a quantidade de dados escritos.\n",
    "            'sparkM_bytesWritten': sparkmeasure_metrics[0]['sum(bytesWritten)'], #Analisam a quantidade de dados escritos.\n",
    "            'sparkM_diskBytesSpilled': sparkmeasure_metrics[0]['sum(diskBytesSpilled)'], #Indicam o quanto de dados foi spillado para o disco ou memória.\n",
    "            'sparkM_memoryBytesSpilled': sparkmeasure_metrics[0]['sum(memoryBytesSpilled)'], #Indicam o quanto de dados foi spillado para o disco ou memória.\n",
    "            'sparkM_shuffleFetchWaitTime': sparkmeasure_metrics[0]['sum(shuffleFetchWaitTime)'], #Tempo gasto esperando dados durante operações de shuffle.\n",
    "            'sparkM_peakExecutionMemory': sparkmeasure_metrics[0]['sum(peakExecutionMemory)'] #A quantidade máxima de memória utilizada durante a execução.\n",
    "    }]\n",
    "\n",
    "        #Insertir dados na tabela de métricas do Big Query\n",
    "        logger.info(\"Inserting metrics data into Big Query\")\n",
    "        #insert_bigquery(metricas)  \n",
    "\n",
    "        logger.info(\"Deletando arquivos do path de origem\")\n",
    "        #delete_files(bucket_name, blob_name)\n",
    "\n",
    "        logger.info(\"Successfully Completed\")\n",
    "\n",
    "        return metricas\n",
    "            \n",
    "    except Exception as e:\n",
    "        return logger.error(f\"Error ingesting table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a43a219e-5e5c-4b17-94f3-9fbe05a786a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Execução do Template de Ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458f3df9-9335-4452-921f-9860c9de4bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 23:36:22,464 - INFO - Start of execution: 2024-09-22 23:36:22.464581\n2024-09-22 23:36:22,466 - INFO - Connection successfully\n2024-09-22 23:36:22,467 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/clientes//\n2024-09-22 23:36:23,294 - INFO - Data loaded successfully\n2024-09-22 23:36:23,296 - INFO - Total time to load data: 0.827346 seconds\n2024-09-22 23:36:23,809 - INFO - Size of loaded data: 1.11 MB\n2024-09-22 23:36:24,177 - INFO - Number of lines loaded 2000\n2024-09-22 23:36:24,179 - INFO - Verificando colunas com string Null ou null e substituindo por None\n2024-09-22 23:36:27,546 - INFO - Mandatory columns are valid.\n2024-09-22 23:36:27,564 - INFO - The table b_cad.clientes exists.\n2024-09-22 23:36:27,565 - INFO - Writing data to the table: clientes\n2024-09-22 23:36:33,159 - INFO - Data recording execution time: 5.59447 seconds\n2024-09-22 23:36:33,161 - INFO - Checking amount of data entered\n2024-09-22 23:36:35,410 - INFO - A total of 48000 rows and a total of 22 columns were inserted into the table\n2024-09-22 23:36:35,416 - INFO - Checking total generated files\n2024-09-22 23:36:36,075 - INFO - Total files generated: 1\n2024-09-22 23:36:36,079 - INFO - End of execution: 2024-09-22 23:36:36.079832\n2024-09-22 23:36:36,081 - INFO - Total execution time: 13.615243\n2024-09-22 23:36:36,082 - INFO - Table clientes ingested successfully\n2024-09-22 23:36:36,084 - WARNING - Check table ingestion, has an ALERT regarding the difference in data found on the load date\n2024-09-22 23:36:36,819 - INFO - Inserting metrics data into Big Query\n2024-09-22 23:36:36,820 - INFO - Deletando arquivos do path de origem\n2024-09-22 23:36:36,821 - INFO - Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela clientes\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_clientes = \"clientes\"\n",
    "db_name_clientes = \"b_cad\"\n",
    "sep = \";\"\n",
    "required_columns = ['nome', 'cpf'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_clientes = ingestion(db_name_clientes, table_name_clientes, sep, required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20e2103-ed66-4a97-bbe4-9491dbbf03ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 23:36:58,277 - INFO - Start of execution: 2024-09-22 23:36:58.277425\n2024-09-22 23:36:58,279 - INFO - Connection successfully\n2024-09-22 23:36:58,280 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/produtos//\n2024-09-22 23:36:59,073 - INFO - Data loaded successfully\n2024-09-22 23:36:59,074 - INFO - Total time to load data: 0.792026 seconds\n2024-09-22 23:36:59,354 - INFO - Size of loaded data: 0.02 MB\n2024-09-22 23:36:59,741 - INFO - Number of lines loaded 50\n2024-09-22 23:36:59,743 - INFO - Verificando colunas com string Null ou null e substituindo por None\n2024-09-22 23:37:01,995 - INFO - Other columns with null values ​​(not required):\n2024-09-22 23:37:01,997 - INFO - Column: limite_credito, Null count: 15\n2024-09-22 23:37:01,998 - INFO - Column: prazo, Null count: 16\n2024-09-22 23:37:01,999 - INFO - Column: contato_suporte, Null count: 11\n2024-09-22 23:37:02,000 - INFO - Column: prazo_carencia, Null count: 6\n2024-09-22 23:37:02,002 - INFO - Mandatory columns are valid.\n2024-09-22 23:37:02,027 - INFO - The table b_cad.produtos exists.\n2024-09-22 23:37:02,028 - INFO - Writing data to the table: produtos\n2024-09-22 23:37:08,181 - INFO - Data recording execution time: 6.152937 seconds\n2024-09-22 23:37:08,192 - INFO - Checking amount of data entered\n2024-09-22 23:37:09,550 - INFO - A total of 150 rows and a total of 17 columns were inserted into the table\n2024-09-22 23:37:09,552 - INFO - Checking total generated files\n2024-09-22 23:37:10,297 - INFO - Total files generated: 1\n2024-09-22 23:37:10,298 - INFO - End of execution: 2024-09-22 23:37:10.298908\n2024-09-22 23:37:10,299 - INFO - Total execution time: 12.021475\n2024-09-22 23:37:10,301 - INFO - Table produtos ingested successfully\n2024-09-22 23:37:10,304 - WARNING - Check table ingestion, has an ALERT regarding the difference in data found on the load date\n2024-09-22 23:37:11,037 - INFO - Inserting metrics data into Big Query\n2024-09-22 23:37:11,038 - INFO - Deletando arquivos do path de origem\n2024-09-22 23:37:11,039 - INFO - Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela produtos\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_produtos = \"produtos\"\n",
    "db_name_produtos = \"b_cad\"\n",
    "sep = \",\"\n",
    "required_columns = ['produto_id', 'nome', 'descricao', 'categoria'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_produtos = ingestion(db_name_produtos, table_name_produtos, sep, required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ca7fb9-88f8-446e-9a9d-7384122576f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 23:37:14,105 - INFO - Start of execution: 2024-09-22 23:37:14.105238\n2024-09-22 23:37:14,107 - INFO - Connection successfully\n2024-09-22 23:37:14,108 - INFO - Starting to load data into the path gs://data-ingestion-bucket-datamaster/table_ingestion_files/clientesxprod//\n2024-09-22 23:37:14,997 - INFO - Data loaded successfully\n2024-09-22 23:37:14,999 - INFO - Total time to load data: 0.889537 seconds\n2024-09-22 23:37:15,371 - INFO - Size of loaded data: 0.58 MB\n2024-09-22 23:37:15,716 - INFO - Number of lines loaded 5000\n2024-09-22 23:37:15,719 - INFO - Verificando colunas com string Null ou null e substituindo por None\n2024-09-22 23:37:16,652 - INFO - Mandatory columns are valid.\n2024-09-22 23:37:16,671 - INFO - The table b_vend.clientesxprod exists.\n2024-09-22 23:37:16,673 - INFO - Writing data to the table: clientesxprod\n2024-09-22 23:37:22,296 - INFO - Data recording execution time: 5.623289 seconds\n2024-09-22 23:37:22,297 - INFO - Checking amount of data entered\n2024-09-22 23:37:23,599 - INFO - A total of 15000 rows and a total of 5 columns were inserted into the table\n2024-09-22 23:37:23,601 - INFO - Checking total generated files\n2024-09-22 23:37:24,238 - INFO - Total files generated: 1\n2024-09-22 23:37:24,241 - INFO - End of execution: 2024-09-22 23:37:24.241336\n2024-09-22 23:37:24,243 - INFO - Total execution time: 10.136083\n2024-09-22 23:37:24,245 - INFO - Table clientesxprod ingested successfully\n2024-09-22 23:37:24,250 - WARNING - Check table ingestion, has an ALERT regarding the difference in data found on the load date\n2024-09-22 23:37:24,989 - INFO - Inserting metrics data into Big Query\n2024-09-22 23:37:24,991 - INFO - Deletando arquivos do path de origem\n2024-09-22 23:37:24,992 - INFO - Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "#Ingestião tabela clientesxprod\n",
    "#Variaveis esperadas para template de carga\n",
    "table_name_clientesxprod = \"clientesxprod\"\n",
    "db_name_clientesxprod = \"b_vend\"\n",
    "sep = \",\"\n",
    "required_columns = ['cliente_id', 'produto_id'] \n",
    "\n",
    "#Template de ingestão e atribuição de métricas\n",
    "metricas_clientesxprod = ingestion(db_name_clientesxprod, table_name_clientesxprod, sep, required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7b39d165-6ef8-4a86-bde5-76b3bf50a00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: 0"
     ]
    }
   ],
   "source": [
    "sparkmeasure_metrics[0]['stageDuration']  #Indica o tempo total que o estágio levou para ser executado. Um tempo longo pode sinalizar gargalos de desempenho.\n",
    "\n",
    "sparkmeasure_metrics[0]['executorRunTime']  #Mostra quanto tempo o executor passou realmente executando a tarefa. É útil para identificar se o tempo total é dominado pela execução ou por outras atividades (como espera).\n",
    "\n",
    "sparkmeasure_metrics[0]['executorCpuTime']  #Refere-se ao tempo que a CPU efetivamente gastou processando a tarefa. Comparar isso com o executorRunTime pode ajudar a identificar se há ineficiências ou tempos de espera\n",
    "\n",
    "sparkmeasure_metrics[0]['jvmGCTime']  #Tempo gasto em coleta de lixo. Se esse valor for alto, pode indicar que o seu aplicativo está consumindo mais memória do que o disponível, levando a pausas frequentes para coleta de lixo.\n",
    "\n",
    "sparkmeasure_metrics[0]['numTasks']  #Indica quantas tarefas foram executadas em paralelo. Se o número de tarefas for baixo, pode indicar que a paralelização não está sendo utilizada de forma eficaz.\n",
    "\n",
    "sparkmeasure_metrics[0]['recordsRead']  #Informam sobre a quantidade de dados lidos. Se esses números forem baixos em comparação com o esperado, pode ser um sinal de problemas na leitura dos dados.\n",
    "\n",
    "sparkmeasure_metrics[0]['bytesRead']  #Informam sobre a quantidade de dados lidos. Se esses números forem baixos em comparação com o esperado, pode ser um sinal de problemas na leitura dos dados.\n",
    "\n",
    "sparkmeasure_metrics[0]['recordsWritten']  #Analisam a quantidade de dados escritos. Se os registros escritos forem significativamente menores do que os lidos, pode indicar uma perda de dados em algum ponto do processo.\n",
    "\n",
    "sparkmeasure_metrics[0]['bytesWritten']  #Analisam a quantidade de dados escritos. Se os registros escritos forem significativamente menores do que os lidos, pode indicar uma perda de dados em algum ponto do processo.\n",
    "\n",
    "sparkmeasure_metrics[0]['diskBytesSpilled']  #Indicam o quanto de dados foi spillado para o disco ou memória, respectivamente. Spill pode ser um sinal de que a operação não está sendo executada de maneira eficiente, resultando em perda de desempenho.\n",
    "\n",
    "sparkmeasure_metrics[0]['memoryBytesSpilled']  #Indicam o quanto de dados foi spillado para o disco ou memória, respectivamente. Spill pode ser um sinal de que a operação não está sendo executada de maneira eficiente, resultando em perda de desempenho.\n",
    "\n",
    "sparkmeasure_metrics[0]['shuffleFetchWaitTime']  #Tempo gasto esperando dados durante operações de shuffle. Altos valores podem indicar um problema de rede ou configuração que afeta a eficiência do processamento.\n",
    "\n",
    "sparkmeasure_metrics[0]['peakExecutionMemory']  #A quantidade máxima de memória utilizada durante a execução. Se o valor estiver perto do limite da memória disponível, pode haver riscos de spill ou falhas.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2226375173427318,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion_bronze_template",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
